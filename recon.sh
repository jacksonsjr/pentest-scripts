#!/bin/bash
# my recon.sh

# Colours
red="\e[31m"
green="\e[32m"
yellow="\e[33m"
reset="\e[0m"

# Get date
date_recon=$(date +%Y%m%d)
# List of directories
pentest_dir=${HOME}/pentest
tools_dir=${pentest_dir}/tools
#wordlists_dir=${pentest_dir}/wordlists

# Verifying if all binaries there are in the system
count=0
for binary in aquatone amass diff dig dirsearch.py dnssearch docker gobuster \
   host httprobe jq nmap subfinder sublist3r.py waybackurls ; do
   if ! command -v "${binary}" > /dev/null 2>&1 ; then
      echo -e "The ${red}${binary} does not exist${reset} on the system!"
      ((count+=1))
   fi
done
if [ "${count}" -gt 0 ]; then
   echo -e "Please, ${yellow}make sure${reset} you got all tools (binaries and scripts)."
   echo -e "You could use the ${yellow}get-tools.sh${reset} to get all binaries and scripts!"
   echo -e "Or you could use the recon.sh ${yellow}inside docker${reset} with Dockerfile in current dir."
   unset count
   exit 1
fi

# List of 3rd party binaries and python scripts to use in this script
amass_bin=$(command -v amass)
aquatone_bin=$(command -v aquatone)
dirsearch_bin=$(command -v dirsearch.py)
dnssearch_bin=$(command -v dnssearch)
docker_bin=$(command -v docker)
gobuster_bin=$(command -v gobuster)
httprobe_bin=$(command -v httprobe)
subfinder_bin=$(command -v subfinder)
sublist3r_bin=$(command -v sublist3r.py)
wayback_bin=$(command -v waybackurls)

# Get the correct path to use chromium with aquatone
for binary in /usr/bin/chromium /usr/bin/chromium-browser; do
    if command -v "${binary}" > /dev/null 2>&1 ; then
       chromium_bin=/usr/bin/chromium
    elif command -v "${binary}" > /dev/null 2>&1 ; then
       chromium_bin=/usr/bin/chromium-browser
    fi
done

# Tools parameters
aquatone_threads=5
#curl_total_processes=50
dirsearch_threads=50
httprobe_timeout=2000
gobuster_threads=50
sublist3r_threads=50
resolver_dns="8.8.8.8"
webdata_total_processes=16
web_extensions="php,asp,aspx,html,htmlx,shtml,txt"

# List of wordlists to use in this script
web_wordlists=("${tools_dir}/dirsearch/db/dicc.txt")
dns_wordlists=()

if [ ${#web_wordlists[@]} -eq 0 ]; then
    echo -e "Please, ${yellow}make sure${reset} you have the default wordlists!"
    exit 1
fi

# Script usage description
usage(){
    (
    echo -e "Usage: ${yellow}$0 -d domain.com${reset}"
    echo "Options: "
    echo -e "\t-d    -  specify a valid domain [needed]"
    echo -e "\t-e    -  specify excluded subdomains after all treated files"
    echo -e "\t\t ${yellow}use -e domain1.com,domain2.com${reset}"
    echo -e "\t-r    -  specify the DNS to resolve"
    echo -e "\t\t ${yellow}use -r 1.1.1.1${reset}"
    echo -e "\t-b    -  if specified the Sublist3r will do brute force, this option take a long time to finish"
    echo -e "\t\t but it brings more results, you need to specify \"yes\" and any other value will be considered as \"no\""
    echo -e "\t\t ${yellow}use -b yes${reset}"
    echo -e "\t-s    -  specify the wordlist to put in dns_wordlist array and execute gobuster and dnssearch brute force"
    echo -e "\t\t this option take a long time to finish, use this own your need, by default the array is empty"
    echo -e "\t\t and not execute gobuster and dnssearch. The success of use those tools is a good wordlist."
    echo -e "\t\t ${yellow}use -s /path/to/wordlist1,/path/to/wordlist2${reset}"
    echo -e "\t-w    -  specity more wordlist to put in web_wordlist by default we use the ${tools_dir}/dirsearch/db/dicc.txt"
    echo -e "\t\t as the first wordlist to enumerate dirs and files from website."
    echo -e "\t\t ${yellow}use -w /path/to/wordlist1,/path/to/wordlist2${reset}"
    echo -e "\t-p    -  this option when set with \"yes\" will use privoxy instance using docker trying to avoid or bypass WAF block"
    echo -e "\t\t and any other value will be considered as \"no\""
    echo -e "\t\t ${yellow}use -p yes${reset}"
    ) 1>&2; exit 1
}

# getopts is used by shell procedures to parse positional parameters.
while getopts ":d:e:r:b:s:w:p:" options; do
    case "${options}" in
        d)
            domain="${OPTARG}"
            ;;
        e)
            set -f
            IFS=","
            excluded+=("${OPTARG}")
	        unset IFS
            ;;
        r)
            unset resolver_dns
            resolver_dns="${OPTARG}"
            ;;
        b)
            brute_sublist3r=$(echo "${OPTARG}" | tr '[:upper:]' '[:lower:]')
            ;;
        s)
            set -f
            IFS=","
            dns_wordlists+=("${OPTARG}")
            unset IFS
            ;;
        w)
            set -f
            IFS=","
            web_wordlists+=("${OPTARG}")
            unset IFS
            ;;
        p)
            use_proxy=$(echo "${OPTARG}" | tr '[:upper:]' '[:lower:]')
            ;;
        *)
            usage
            ;;
    esac
done
# OPTIND The index of the next argument to be processed by the getopts builtin command (see bash man page).
shift $((OPTIND - 1))

valid_domain=$(host -t A "${domain}" > /dev/null 2>&1; echo $?)

if [ -z "${domain}" ] || [ "${valid_domain}" -ne 0 ]; then
    usage
    exit 1
else
    # Create all dirs necessaries to report and recon 
    if [ -d "./${domain}" ]; then
        echo "This is a known target." 
    fi
    echo -n "Preparing the directories structure to work... "
    mkdir -p ./"${domain}"/{log,recon_"${date_recon}"}
    log_dir="./${domain}/log"
    recon_dir="./${domain}/recon_${date_recon}"
    mkdir -p "${recon_dir}"/{tmp,report,wayback-data,aquatone,web-data}
    tmp_dir="${recon_dir}/tmp"
    report_dir="${recon_dir}/report"
    wayback_dir="${recon_dir}/wayback-data"
    aquatone_data="${recon_dir}/aquatone"
    web_data_dir="${recon_dir}/web-data"
    echo "Done!"
fi

message() {
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} ${green}Recon finished on${reset} ${yellow}${domain}${reset}${green}!${reset}"
    echo -e "\t ${green}Consider to use recon-ng and theHarvester to help get more assets!${reset}"
    echo -e "\t ${green}Use Shoda.io, Censys and others.${reset}"
}

subdomains_recon(){
    if [ -d "${tmp_dir}" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} ${green}Recon started on${reset} ${yellow}${domain}${reset}${green}!${reset}"
        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing amass... "
        ${amass_bin} enum -d "${domain}" > "${tmp_dir}/amass_output.txt" 2>&1
        #${amass_bin} enum --passive -d "${domain}" > "${tmp_dir}/amass_passive_output.txt" 2>&1
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing certspotter... "
        curl -s https://certspotter.com/api/v0/certs\?domain="${domain}" | jq '.[].dns_names[]' | \
            sed 's/\"//g' | sed 's/\*\.//g' | sort -u | grep "${domain}" >> "${tmp_dir}/certspotter_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing crt.sh... "
        curl -s https://crt.sh/?q=%."${domain}"\&output=json | jq -r '.[].name_value' | \
            sed 's/\*\.//g' | sort -u >> "${tmp_dir}/crtsh_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing dns bufferover... "
        curl -s https://dns.bufferover.run/dns?q="${domain}" | grep "${domain}" \
            | sed -e "s/\"//" -e "s/\",$//" -e "s/\"//" -e "s/\t\t//" \
            | grep -E "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" | awk -F"," '{print $2}' \
            | sort -u >> "${tmp_dir}/dnsbufferover_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing subfinder... "
        ${subfinder_bin} -silent -d "${domain}" > "${tmp_dir}/subfinder_output.txt"
        echo "Done!"
        
        if [ -n "${brute_sublist3r}" ] && [ "${brute_sublist3r}" == "yes" ]; then
            echo -e "${red}Warning:${reset} Sublist3r take almost 30 minutes to be executed!"
            echo -e "\t If you find yourself taking longer than expected: "
            echo -e "\t ${yellow}>>${reset} check for a zombie python process;"
            echo -e "\t ${yellow}>>${reset} stop the script;"
            echo -e "\t ${yellow}>>${reset} change the default parameters of the Sublist3r in the script to a lower value;"
            echo -e "\t ${yellow}>>${reset} execute the script again;"
            echo -e "\t ${yellow}>>${reset} if the problem persists, check your internet connection."
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
            "${sublist3r_bin}" -n -d "${domain}" -b -t "${sublist3r_threads}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
            echo "Done!"
        else
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
            ${sublist3r_bin} -n -d "${domain}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
            echo "Done!"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} We will execute gobuster and dnssearch ${#dns_wordlists[@]} time(s)."
            for list in "${dns_wordlists[@]}"; do
                index=$(printf "%s\n" "${dns_wordlists[@]}" | grep -En "^${list}$" | awk -F":" '{print $1}')
                if [ -s "${list}" ]; then
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}... "
                    ${gobuster_bin} dns -z -q -r "${resolver_dns}" -t "${gobuster_threads}" \
                        -d "${domain}" -w "${list}" > "${tmp_dir}/gobuster_dns_output_${index}.txt" 2>&1
                    ${dnssearch_bin} -consumers 600 -domain "${domain}" \
                        -wordlist "${list}" > "${tmp_dir}/dnssearch_output_${index}.txt" 2>&1
                    echo "Done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}, error: ${list} does not exist or is empty!"
                    continue
                fi
                unset index
            done
            unset list
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution of gobuster and dnssearch is done."
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script"
        exit 1
    fi

    IFS=" " read -r -a zone_transfer <<< "$(dig ns "${domain}" | grep "^${domain}\." | awk '{print $5}' | sed -e 's/\.$//')"
    #zone_transfer=($(dig ns "${domain}" | grep "^${domain}\." | awk '{print $5}' | sed -e 's/\.$//'))
    for ns in "${zone_transfer[@]}"; do
        if ! dig axfr "@${ns}" "${domain}" 2> /dev/null | \
            grep -Ei "Transfer failed.|servers could be reached|timed out." > /dev/null 2>&1; then
            dig axfr "@${ns}" "${domain}" >> "${tmp_dir}/zone_transfer.txt"
        fi
    done
}

adjust_files(){
    if [ -d "${tmp_dir}" ] && [ -d "${report_dir}" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Putting all domains only in one file and getting IPs block!!!"

        if [ -s "${tmp_dir}/amass_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass output... "
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_output.txt" | \
                grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            grep -A 1000 "OWASP Amass.*OWASP/Amass" "${tmp_dir}/amass_output.txt" >> "${report_dir}/amass_blocks_output.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -vE "^([0-9a-zA-Z]{1,4}:)" | sed '/0.0.0.0\/0/d' >> "${report_dir}/ips_blocks.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -E "^([0-9a-zA-Z]{1,4}:)" >> "${report_dir}/ipv6_blocks.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/amass_passive_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass passive output... "
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_passive_output.txt" | \
                grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/certspotter_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the certspotter output... " 
            sort -u "${tmp_dir}/certspotter_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/crtsh_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the crtsh output... " 
            sort -u "${tmp_dir}/crtsh_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi
        
        if [ -s "${tmp_dir}/dnsbufferover_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the dns bufferover output... " 
            sort -u "${tmp_dir}/dnsbufferover_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/subfinder_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the subfinder output... " 
            sort -u "${tmp_dir}/subfinder_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/sublist3r_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the sublist3r output... "
            sed -i -e 's/<BR>/\n/g' -e '1,10d' "${tmp_dir}/sublist3r_output.txt" 
            grep -Ev "Searching.*in|Starting.*subbrute|Total.*Found|Error:.*requests|Finished.*Enumeration|Warning:.*resolvers.txt"  \
                "${tmp_dir}/sublist3r_output.txt" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the gobuster dns output... "
            files=("${tmp_dir}"/gobuster_dns_output_*.txt)
            for file in "${files[@]}"; do
                if [ -s "${file}" ]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
                else
                    echo " "
                    echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                    continue
                fi
            done
            unset file
            unset files
            echo "Done!"

            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the dnsearch output... "
            files=("${tmp_dir}"/gobuster_dns_output_*.txt)
            for file in "${files[@]}"; do
                if [ -s "${file}" ]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
                else
                    echo " "
                    echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                    continue
                fi
            done
            unset file
            unset files
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/domains_tmp.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains... "
            # Removing duplicated subdomains
            if tr '[:upper:]' '[:lower:]' < "${tmp_dir}/domains_tmp.txt" | sed -e 's/^\.//' -e 's/^-//' -e 's/\.\./\./' | \
                sed -e 's/^http:\/\///' -e 's/^https:\/\///' -e 's/ //' | sort -u > "${report_dir}/domains_all.txt" ; then
                # Domains and subdomains resolution
                while IFS= read -r subdomain; do
                    host -t A "${subdomain}" >> "${tmp_dir}/domains_resolution.txt"
                done < "${report_dir}/domains_all.txt"
                unset subdomain
                # Sorting out...
                if [ -s "${tmp_dir}/domains_resolution.txt" ] ; then
                    sed -i -e 's/\.$//' "${tmp_dir}/domains_resolution.txt"
                    # Domains with IPs
                    grep "has address" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $1"\t"$4}' >> "${report_dir}/domains_ips.txt"
                    # Domains aliases
                    grep "alias" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $1"\t"$6}' >> "${report_dir}/domains_aliases.txt"
                    # Unavailable domains
                    grep -E "NXDOMAIN|SERVFAIL" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $2}' >> "${report_dir}/domains_null.txt"
                else
                    echo " " 
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Something got wrong on domains_all.txt resolution, please fix it."
                    exit 1
                fi
                # Remove domains that does not work from domains_all.txt
                if [ -s "${report_dir}/domains_null.txt" ]; then
                    while IFS= read -r subdomain; do
                        sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"  
                    done < "${report_dir}/domains_null.txt"
                    unset subdomain
                fi
            else
                echo " "
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains!"
                exit 1
            fi     
            echo "Done!"
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The file with all domains from initial recon does not exist or is empty."
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Look all files from initial recon in ${tmp_dir} and fix the problem!"
            exit 1
        fi

        if [ ${#excluded[@]} -gt 0 ] && [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Excluding the subdomains from command line option... "
            for subdomain in "${excluded[@]}" ;do
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_ips.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_aliases.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_null.txt"
            done
            unset subdomain
            echo "Done!"
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

diff_domains(){
    if [ -d "${report_dir}" ]; then 
        if [ -s "${report_dir}/domains_all.txt" ]; then
            oldest_domains_all=$(find "./${domain}" -name domains_all.txt -type f | sort -u | grep -v "${date_recon}" | tail -n1)
            oldest_domains_web=$(find "./${domain}" -name domains_web.txt -type f | sort -u | grep -v "${date_recon}" | tail -n1)
            if [[ -n "${oldest_domains_all}" ]] && [[ -n "${oldest_domains_web}" ]]; then
                if cmp -s "${oldest_domains_all}" "${report_dir}/domains_all.txt"; then
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting the difference between files to improve the time running recon.sh... "
                    diff -au "${oldest_domains_all}" "${report_dir}/domains_all.txt" | grep -E '^\+' | sed -e '/+++/d' -e 's/^+//' >> "${report_dir}/domains_diff.txt"
                    if [ -s "${report_dir}/domains_diff.txt" ]; then
                        if mv "${report_dir}/domains_all.txt" "${report_dir}/domains_all.${date_recon}"; then
                            cp "${report_dir}/domains_diff.txt" "${report_dir}/domains_all.txt"
                        else
                            echo " "
                            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error during move domains_all.txt to domains_all.old."
                            echo -e "\t Stopping the script!"
                            exit 1
                        fi
                    else
                        echo " "
                        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There isn't any changes since last execution."
                        echo -e "\t Stopping the script!"
                        exit 1
                    fi
                    echo "Done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The files are same since last execution of recon.sh script!"
                    echo -e "\t Stopping the script!"
                    exit 1
                fi
            else
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There is nothing different between, this execution and the last one!"
            fi
            unset data
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} diff_domains function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
            exit 1
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

nmap_ips(){
    if [ -d "${report_dir}" ]; then
        if [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting subdomains IP to use with nmap... "
            awk '{print $2}' "${report_dir}/domains_ips.txt" | sort -u >> "${report_dir}/nmap_ips.txt"
            echo "Done!"
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} nmap_ips function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
            exit 1
        fi
        if [ -s "${report_dir}/ips_blocks.txt" ]; then
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting IPs from blocks with nmap -sn \"block\"..."
            count=0
            while IFS= read -r block; do
                block_file=nmap_$(echo "${block}" | sed -e 's/\//_/').txt
                cidr=$(echo "${block}" | awk -F'/' '{print $2}')
                if [[ ${cidr} -ge 21 ]]; then
                    nmap -sn "${block}" --exclude 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 --max-retries 3 --host-timeout 3 \
                        | grep -E "Nmap.*for" | awk '{print $6}' | sed -e 's/(//' -e 's/)//' > "${report_dir}"/"${block_file}"
                    sed -i '/^$/d' "${report_dir}/${block_file}"
                    (( count+=1 ))
                else
                    continue
                fi
                unset block_file
                unset cidr
            done < "${report_dir}/ips_blocks.txt"
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Got IP blocks, done!"
            if [[ ${count} -lt $(wc -l "${report_dir}/ips_blocks.txt" | awk '{print $1}') ]]; then
                echo -e "${red}Warning:${reset} Just ${count} block(s) were scanned, please look at ${report_dir}/ips_blocks.txt"
                echo -e "\t and nmap blocks files to know what were excluded blocks."
            fi
            unset count
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

hosts_alive(){
    if [ -d "${report_dir}" ]; then
        if [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Testing subdomains to know if it is or have web application... "
            while IFS= read -r subdomain; do
                echo "${subdomain}" | ${httprobe_bin} -t "${httprobe_timeout}" \
                    -p http:8080 -p http:8443 -p http:8081 -p http:8010 -p http:8085 -p http:8086 \
                    -p http:8087 -p http:8008 | sort -u >> "${report_dir}/domains_web.txt" 
            done < "${report_dir}/domains_all.txt"
            unset subdomain
            echo "Done!"

            if cp "${report_dir}/domains_all.txt" "${report_dir}/domains_infra.txt"; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Separating infrastructure from web application... "
                while IFS= read -r line; do
                    subdomain=$(echo "${line}" | sed -e "s/http:\/\///" -e "s/https:\/\///" | awk -F":" '{print $1}')
                    if grep "${subdomain}" "${report_dir}"/domains_infra.txt > /dev/null 2>&1 ; then
                        sed -i "/^${subdomain}$/d" "${report_dir}"/domains_infra.txt  
                    else
                        continue
                    fi
                unset subdomain
                done < "${report_dir}/domains_web.txt"
                echo "Done!"
            fi
            
            if [ -s "${report_dir}/domains_web.txt" ] && [ -s "${report_dir}/domains_infra.txt" ]; then
                echo -e "\t We have $(wc -l "${report_dir}/domains_web.txt" | awk '{print $1}') Web Applications URLs."
                echo -e "\t We have $(wc -l "${report_dir}/domains_infra.txt" | awk '{print $1}') Infrastructure domains."
            fi

        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} hosts_alive function error: the ${report_dir}/domains_all.txt does not exist or is empty."
            exit 1
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

web_data(){
    if [ $# != 1 ]; then
        echo "Please, especify just 1 file to get URL from."
        exit 1
    else
        urls_file=$1
        if [ -s "${urls_file}" ]; then
            if [ -d "${report_dir}" ] && [ -d "${wayback_dir}" ] && [ -d "${web_data_dir}" ]; then
                if [ ${#web_wordlists[@]} -gt 0 ]; then
                    echo -e "${red}Warning:${reset} It can take a long time to execute!"
                    echo -e "\t We have ${#web_wordlists[@]} wordlists and $(wc -l "${urls_file}" | awk '{print $1}') urls to scan." 
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function will use ${#web_wordlists[@]} wordlists with gobuster and dirsearch... "
                    for list in "${web_wordlists[@]}"; do
                        index=$(printf "%s\n" "${web_wordlists[@]}" | grep -En "^""${list}""$" | awk -F":" '{print $1}')
                        if [ -s "${list}" ]; then
                            count=1
                            proxy_port=8118
                            while IFS= read -r url; do
                                # Mounting the file names
                                name="$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")"
                                file_gobuster="gobuster_${name}_${index}.txt"
                                file_dirsearch="dirsearch_${name}_${index}.txt"
                                if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                                    name_instance="privoxy-${count}"
                                    # Running a docker proxy instance to try bypass some protection system like WAF
                                    ${docker_bin} run -d --rm --name "${name_instance}" -p "${proxy_port}:8118" privoxy > /dev/null
                                    proxy_ip=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' "${name_instance}")
                                    #while [ -z "${Congratulations}" ]; do
                                    #    Congratulations=$(curl -m 2 -s --proxy "${proxy_ip}:8118" 'https://check.torproject.org/' 2> /dev/null | grep -m1 "Congratulations" | awk '{print $1}')
                                    #done                                    
                                    # Skipping the specific wordlist from dirsearch on gobuster
                                    if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" > /dev/null 2>&1 ; then
                                    ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                        -w "${list}" --proxy "${proxy_ip}:8118" \
                                        -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    else
                                        ${gobuster_bin} dir --delay 300ms -k -z -t "${gobuster_threads}" -x "${web_extensions}" -w "${list}" \
                                            -u "${url}" --proxy "${proxy_ip}:8118" > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" --proxy "${proxy_ip}:8118" \
                                            -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    fi
                                    while [[ "$(pgrep -acf "[d]ocker-proxy")" -eq "${webdata_total_processes}" ]]; do
                                        instances=($(${docker_bin} ps --format '{{.Names}}' | grep "privoxy-"))
                                        ip_proxies=($(pgrep -fa dirsearch.py | awk '{print $12}' | cut -d":" -f1))
                                        for ip in "${ip_proxies}"; do
                                            for instance in "${instances[@]}"; do
                                                [[ "$(${docker_bin} inspect --format '{{ .NetworkSettings.IPAddress }}' "${instance}")" == "${ip}" ]] && \
                                                    instances=( "${instances[@]}/${instance}" )
                                            done
                                        done
                                        if [[ "${#instances[@]}" -gt 0 ]]; then
                                            for name_instance in "${instances[@]}"; do
                                                ${docker_bin} stop "${name_instance}" > /dev/null 2>&1
                                            done
                                        fi
                                        unset instances
                                        unset ip_proxies
                                    done
                                else
                                    # Skipping the specific wordlist from dirsearch on gobuster
                                    if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" > /dev/null 2>&1 ; then
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    else
                                        ${gobuster_bin} dir --delay 300ms -k -z -t "${gobuster_threads}" -x "${web_extensions}" -w "${list}" \
                                            -u "${url}" > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    fi
                                    while [[ "$(pgrep -acf "[d]irsearch|[g]obuster")" -eq "${webdata_total_processes}" ]]; do
                                        sleep 1
                                    done
                                fi
                                #if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                                #else
                                #fi
                                (( count+=1 ))
                                (( proxy_port+=1 ))
                                unset file_dirsearch
                                unset file_gobuster
                                unset name
                                unset url
                            done < "${urls_file}"
                            unset count
                            unset proxy_port
                            unset proxy_ip
                        else
                            echo " "
                            echo -e "\t ${red}Error:${reset} ${list} does not exist or is empty!"
                            continue
                        fi
                        unset index
                        unset list
                    done
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Waiting the dirsearch and/or gobuster finish... "
                    #while $(ps aux | grep -E "[d]irsearch.py|[g]obuster" > /dev/null); do
                    while pgrep -f dirsearch > /dev/null || pgrep -f gobuster > /dev/null; do
                        sleep 1
                    done
                    echo "Done!"
                    if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Stopping the privoxy instances... "
                        IFS=" " read -r -a instance_names <<< "$(docker ps --format '{{.Names}}' | grep "privoxy-")"
                        ${docker_bin} stop "${instance_names[@]}"
                        echo "Done!"
                    fi
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function is done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} dirseach/goboster web_data function error: array of wordlists is empty. Stopping the script"
                    exit 1
                fi
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing wayback... "
                while IFS= read -r url; do
                    name=$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")
                    file="wayback_${name}.txt"
                    echo "${url}" | ${wayback_bin} > "${wayback_dir}/${file}" 2>&1
                    unset file
                done < "${urls_file}"
                unset url
                echo "Done!"
            else
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
                unset urls_file
                exit 1
            fi
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the ${urls_file} exist and isn't empty."
            unset urls_file
        fi
        unset urls_file
    fi
}

cleanup_web_data_files(){
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up dirsearch files... "
    files=("${web_data_dir}"/dirsearch*.txt)
    for file in "${files[@]}"; do
        sed -i -e 's/.\[4.m//g' -e 's/.\[3.m//g' -e 's/.\[1K.\[0G/\n/g' "${file}" 2> /dev/null
        sed -i -e 's/.\[1m//g' -e 's/.\[0m//g' "${file}" 2> /dev/null
        sed -i -e '/Last request to/d' "${file}" 2> /dev/null
        #sed -i -e '/^$/d' "${file}" 2> /dev/null
    done 
    echo "Done!"
    unset file
    unset files
       
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up gobuster files... "
    files=("${web_data_dir}"/gobuster*.txt)
    for file in "${files[@]}"; do
        sed -i "s/^..\[2K//" "${file}" 2> /dev/null
    done 
    echo "Done!"
    unset file
    unset files
}

robots_txt(){
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for new URLs on robots.txt... "
    files=("${web_data_dir}"/*.txt)
    for file in "${files[@]}"; do
        if grep robots.txt "${file}" > /dev/null && [ -s "${file}" ] ; then
            target=$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//') 
            for url in $(curl -s "${target}"/robots.txt | grep -Ev "User-agent: *" | awk '{print $2}' | sed -e "/^\/$/d"); do
                echo "${target}${url}" >> "${report_dir}/robots_urls.txt"
                sed -i -e 's/\r//g' -e 's/\/$//g' "${report_dir}/robots_urls.txt"
            done
        fi
        unset target
        unset file
    done 
    echo "Done!"
    unset files
}

aquatone_function(){
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Starting aquatone scan... "
    for file in "${report_dir}/domains_web.txt" "${report_dir}/robots_urls.txt"; do
        if [ -s "${file}" ]; then
            aquatone_log=${tmp_dir}/aquatone_$(basename "${file}" | awk -F'.' '{print $1}').log
            aquatone_files_dir=${aquatone_data}/$(basename "${file}" | awk -F'.' '{print $1}')
            if [ ! -d "${aquatone_files_dir}" ]; then
                if mkdir -p "${aquatone_files_dir}" ; then
                    "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
                else
                    echo " "
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Something got wrong, wasnt possible create directory ${aquatone_files_dir}."
                    echo -e "\t Please, look what got wrong and run the script again. Stopping the script!"
                    exit 1
                fi
            else
                "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
            fi
        else
            echo -e "\t The ${file} does not exist or is empty!"
        fi
    done
    unset aquatone_log
    unset aquatone_files_dir
    unset file
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Finish aquatone scan!"
}

rebuild_git(){
    if [ ! -x "${PWD}/git-dumper/git-dumper.py" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The git-dumper.py script does not exist, exiting!"
        exit 1
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for git repository on web_data directory..."
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} This function has no 100% guaranty to completely recover the .git repository."
        #echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Especially if the repository has been compressed into \"pack\"-files, it may fail."
        count=1
        files=("${web_data_dir}"/*.txt)
        proxy_port=8118
        for file in "${files[@]}"; do
            #git_dir="${report_dir}/$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//' -e 's/http:\/\///' -e 's/https:\/\///')/.git"
            target_dir="${report_dir}/$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//' -e 's/http:\/\///' -e 's/https:\/\///')"
            target=$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//')
            if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                name_instance="privoxy-${count}"
                # Running a docker proxy instance to try bypass some protection system like WAF
                ${docker_bin} run -d --rm --name "${name_instance}" -p "${proxy_port}:8118" privoxy > /dev/null
                proxy_ip=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' "privoxy-${count}")
                while [ -z "${Congratulations}" ]; do
                    Congratulations=$(curl -m 2 -s --proxy "${proxy_ip}:8118" 'https://check.torproject.org/' 2> /dev/null | grep -m1     "Congratulations" | awk '{print $1}')
                done
                if grep ".git/config" "${file}" > /dev/null && [ -s "${file}" ] && \
                        [[ "200" -eq "$(curl --proxy "${proxy_ip}:8118" -o /dev/null -s -w "%{http_code}\n" "${target}/.git/config")" ]] ; then
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Found .git on ${green}${target}${reset}!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Creating the .git directory structure for ${green}${target}${reset}... "
                    echo "Done!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading the static and objects files from repository... "
                    "${PWD}/git-dumper/git-dumper.py" --proxy "${proxy_ip}:8118" "${target}" "${target_dir}" > /dev/null 2>&1
                    echo "Done!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading files from repository... "
                    dir_origem="${PWD}"
                    cd "${target_dir}" || exit
                    for repo_file in $(git ls-files); do
                        repo_file_dir=$(dirname "${repo_file}")
                        if [[ ! -d "${repo_file_dir}" ]] && [[ "${repo_file_dir}" != "." ]]; then
                            mkdir -p "${repo_file_dir}"
                        fi
                        curl -L -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" \
                            --proxy "${proxy_ip}:8118" -f -s -k --max-time 60 "${target}/${repo_file}" -o "${repo_file}" &
                    done
                    while pgrep -f curl > /dev/null; do
                        sleep 1
                    done
                    echo "Done!"
                    cd "${dir_origem}" || exit
                fi
                (( count+=1 ))
                (( proxy_port+=1 ))
            docker stop ${name_instance}
            else
                if grep ".git/config" "${file}" > /dev/null && [ -s "${file}" ] && \
                        [[ "200" -eq "$(curl -o /dev/null -s -w "%{http_code}\n" "${target}/.git/config")" ]] ; then
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Found .git on ${green}${target}${reset}!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Creating the .git directory structure for ${green}${target}${reset}... "
                    echo "Done!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading the static and objects files from repository... "
                    "${PWD}/git-dumper/git-dumper.py" "${target}" "${target_dir}" > /dev/null 2>&1
                    echo "Done!"
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading files from repository... "
                    dir_origem="${PWD}"
                    cd "${target_dir}" || exit
                    for repo_file in $(git ls-files); do
                        repo_file_dir=$(dirname "${repo_file}")
                        if [[ ! -d "${repo_file_dir}" ]] && [[ "${repo_file_dir}" != "." ]]; then
                            mkdir -p "${repo_file_dir}"
                        fi
                        curl -L -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" \
                            -f -s -k --max-time 60 "${target}/${repo_file}" -o "${repo_file}" &
                    done
                    while pgrep -f curl > /dev/null; do
                        sleep 1
                    done
                    echo "Done!"
                    cd "${dir_origem}" || exit
                fi
            fi
        done
    fi
}

#report(){
#   Falta criar a parte de report, página web, subir o server em python, etc...
#}

# Initiating the recon.sh script
(
# Show the directory structure
echo "./${domain}"
echo -e "  ├── log (${yellow}log dir for recon.sh execution${reset})"
echo -e "  └── $(echo "${recon_dir}" | awk -F "/" '{print $3}')"
echo -e "      ├── aquatone (${yellow}aquatone output files${reset})"
echo -e "      ├── report (${yellow}adjust function output files${reset})"
echo -e "      ├── tmp (${yellow}subdomains recon tmp files${reset})"
echo -e "      ├── wayback-data (${yellow}web data function for waybackurl output${reset})"
echo -e "      └── web-data (${yellow}web data function for gobuster and dirsearch output${reset})"
echo "Directories created."
echo -e "${red}Attention:${reset} The output from all tools used here will be placed in background and treated later."
echo -e "\t   If you need look the output in execution time, you need to \"tail\" the files."

#subdomains_recon
#adjust_files
#diff_domains
#nmap_ips
#hosts_alive
web_data "${report_dir}/domains_web.txt"
cleanup_web_data_files
robots_txt
web_data "${report_dir}/robots_urls.txt"
cleanup_web_data_files
aquatone_function
rebuild_git
#report
message
) 2>&1 | tee -a "${log_dir}/recon_${date_recon}.log"
