#!/bin/bash

# my recon.sh

# Colours
red="\e[31m"
green="\e[32m"
yellow="\e[33m"
reset="\e[0m"

# Get date
date_recon=$(date +%Y%m%d)

# List of directories
pentest_dir=${HOME}/pentest
tools_dir=${pentest_dir}/tools
#wordlists_dir=${pentest_dir}/wordlists

# Verifying if all binaries there are in the system
count=0
for binary in aquatone amass diff dirsearch.py dnsrecon.py dnssearch docker \
  gobuster host httprobe jq massdns nmap sublist3r.py waybackurls ; do
  if ! command -v "${binary}" > /dev/null 2>&1 ; then
      echo -e "The ${red}${binary} does not exist${reset} on the system!"
      ((count+=1))
  fi
done
if [ ${count} -gt 0 ]; then
  echo -e "Please, ${yellow}make sure${reset} you got all tools (binaries and scripts)."
  echo -e "You could use the ${yellow}prepare-recon.sh${reset} to get all binaries and scripts!"
  echo -e "Or you could use the recon.sh ${yellow}inside docker${reset} with Dockerfile in current dir."
  unset count
  exit 1
fi

# List of 3rd party binaries and python scripts to use in this script
amass_bin=$(command -v amass)
aquatone_bin=$(command -v aquatone)
dirsearch_bin=$(command -v dirsearch.py)
#dnsrecon_bin=$(command -v dnsrecon.py)
dnssearch_bin=$(command -v dnssearch)
docker_bin=$(command -v docker)
gobuster_bin=$(command -v gobuster)
httprobe_bin=$(command -v httprobe)
#massdns_bin=$(command -v massdns)
sublist3r_bin=$(command -v sublist3r.py)
wayback_bin=$(command -v waybackurls)

# Get the correct path to use chromium with aquatone
for binary in /usr/bin/chromium /usr/bin/chromium-browser; do
  if  command -v "${binary}" > /dev/null 2>&1 ; then
      chromium_bin=/usr/bin/chromium
  elif  command -v "${binary}" > /dev/null 2>&1 ; then
      chromium_bin=/usr/bin/chromium-browser
  fi
done

# Tools parameters
aquatone_threads=5
dirsearch_threads=50
gobuster_threads=50
sublist3r_threads=40
resolver_dns="8.8.8.8"
web_extensions="php,asp,aspx,html,htmlx,shtml,txt"

# List of wordlists to use in this script
web_wordlists=("${tools_dir}/dirsearch/db/dicc.txt")
dns_wordlists=()

if [ ${#web_wordlists[@]} -eq 0 ]; then
  echo -e "Please, ${yellow}make sure${reset} you have the default wordlists!"
  exit 1
fi

# Script usage description
usage(){
  (
  echo -e "Usage: ${yellow}$0 -d domain.com${reset}"
  echo "Options: "
  echo -e "\t-d    -  specify a valid domain [needed]"
  echo -e "\t-e    -  specify excluded subdomains after all treated files"
  echo -e "\t\t ${yellow}use -e domain1.com,domain2.com${reset}"
  echo -e "\t-r    -  specify the DNS to resolve"
  echo -e "\t\t ${yellow}use -r 1.1.1.1${reset}"
  echo -e "\t-b    -  if specified the Sublist3r will do brute force, this option take a long time to finish"
  echo -e "\t\t but it brings more results, you need to specify \"yes\" and any other value will be considered as \"no\""
  echo -e "\t\t ${yellow}use -b yes${reset}"
  echo -e "\t-s    -  specify the wordlist to put in dns_wordlist array and execute gobuster and dnssearch brute force"
  echo -e "\t\t this option take a long time to finish, use this own your need, by default the array is empty"
  echo -e "\t\t and not execute gobuster and dnssearch. The success of use those tools is a good wordlist."
  echo -e "\t\t ${yellow}use -s /path/to/wordlist1,/path/to/wordlist2${reset}"
  echo -e "\t-w    -  specity more wordlist to put in web_wordlist by default we use the ${tools_dir}/dirsearch/db/dicc.txt"
  echo -e "\t\t as the first wordlist to enumerate dirs and files from website."
  echo -e "\t\t ${yellow}use -w /path/to/wordlist1,/path/to/wordlist2${reset}"
  echo -e "\t-p    -  this options when set with \"yes\" will use privoxy instance using docker"
  echo -e "\t\t trying to avoid or bypass WAF block"
  echo -e "\t\t ${yellow}use -p yes${reset}"
  ) 1>&2; exit 1
}

# getopts is used by shell procedures to parse positional parameters.
while getopts ":d:e:r:b:s:w:p:" options; do
  case "${options}" in
      d)
          domain="${OPTARG}"
          ;;
      e)
          set -f
          IFS=","
          excluded+=("${OPTARG}")
	      unset IFS
          ;;
      r)
          unset resolver_dns
          resolver_dns="${OPTARG}"
          ;;
      b)
          brute_sublist3r=$(echo "${OPTARG}" | tr '[:upper:]' '[:lower:]')
          ;;
      s)
          set -f
          IFS=","
          dns_wordlists+=("${OPTARG}")
          unset IFS
          ;;
      w)
          set -f
          IFS=","
          web_wordlists+=("${OPTARG}")
          unset IFS
          ;;
      p)
          use_proxy=$(echo "${OPTARG}" | tr '[:upper:]' '[:lower:]')
          ;;
      *)
          usage
          ;;
    esac
done
# OPTIND The index of the next argument to be processed by the getopts builtin command (see bash man page).
shift $((OPTIND - 1))

valid_domain=$(host -t A "${domain}" > /dev/null 2>&1; echo $?)

if [ -z "${domain}" ] || [ "${valid_domain}" -ne 0 ]; then
  usage
  exit 1
else
  # Create all dirs necessaries to report and recon 
  if [ -d "./${domain}" ]; then
      echo "This is a known target." 
  fi
  echo -n "Preparing the directories structure to work... "
  mkdir -p ./"${domain}"/{log,recon_"${date_recon}"}
  log_dir="./${domain}/log"
  recon_dir="./${domain}/recon_${date_recon}"
  mkdir -p "${recon_dir}"/{tmp,report,wayback-data,aquatone,web-data}
  tmp_dir="${recon_dir}/tmp"
  report_dir="${recon_dir}/report"
  wayback_dir="${recon_dir}/wayback-data"
  aquatone_data="${recon_dir}/aquatone"
  web_data_dir="${recon_dir}/web-data"
  echo "Done!"
  # Show the directory structure
  echo "./${domain}"
  echo -e "  ├── log (${yellow}log dir for recon.sh execution${reset})"
  echo -e "  └── $(echo "${recon_dir}" | awk -F "/" '{print $3}')"
  echo -e "      ├── aquatone (${yellow}aquatone output files${reset})"
  echo -e "      ├── report (${yellow}adjust function output files${reset})"
  echo -e "      ├── tmp (${yellow}subdomains recon tmp files${reset})"
  echo -e "      ├── wayback-data (${yellow}web data function for waybackurl output${reset})"
  echo -e "      └── web-data (${yellow}web data function for gobuster and dirsearch output${reset})"
  echo "Directories created."
  # Log
  execution_log="${log_dir}/recon_${date_recon}.log"
fi

message() {
  echo -e "${yellow}$(date +%H:%M)${reset} ${green}>>${reset} ${green}Recon finished on${reset} ${yellow}${domain}${reset}${green}!${reset}"
  echo -e "\t ${green}Consider to use recon-ng and theHarvester to help get more assets!${reset}"
  echo -e "\t ${green}Use Shoda.io, Censys and others.${reset}"
}

subdomains_recon(){
  if [ -d "${tmp_dir}" ]; then
      echo -e "${red}Attention:${reset} The output from all tools used here will be placed in background and treated later."
      echo -e "\t   If you need look the output in execution time, you need to \"tail\" the files."
      echo -e "${green}Recon started on${reset} ${yellow}${domain}${reset}${green}!${reset}"
      echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing amass... "
      ${amass_bin} enum -d "${domain}" > "${tmp_dir}/amass_output.txt" 2>&1
      #${amass_bin} enum --passive -d "${domain}" > "${tmp_dir}/amass_passive_output.txt" 2>&1
      echo "Done!"

      if [ -n "${brute_sublist3r}" ] && [ "${brute_sublist3r}" == "yes" ]; then
          echo -e "${red}Warning:${reset} Sublist3r take almost 30 minutes to be executed!"
          echo -e "\t If you find yourself taking longer than expected: "
          echo -e "\t ${yellow}>>${reset} check for a zombie python process;"
          echo -e "\t ${yellow}>>${reset} stop the script;"
          echo -e "\t ${yellow}>>${reset} change the default parameters of the Sublist3r in the script to a lower value;"
          echo -e "\t ${yellow}>>${reset} execute the script again;"
          echo -e "\t ${yellow}>>${reset} if the problem persists, check your internet connection."
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
          "${sublist3r_bin}" -n -d "${domain}" -b -t "${sublist3r_threads}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
          echo "Done!"
      else
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
          ${sublist3r_bin} -n -d "${domain}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
          echo "Done!"
      fi

      echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing certspotter... "
      curl -s https://certspotter.com/api/v0/certs\?domain="${domain}" | jq '.[].dns_names[]' | \
          sed 's/\"//g' | sed 's/\*\.//g' | sort -u | grep "${domain}" >> "${tmp_dir}/certspotter_output.txt"
      echo "Done!"

      echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing crt.sh... "
      curl -s https://crt.sh/?q=%."${domain}"\&output=json | jq -r '.[].name_value' | \
          sed 's/\*\.//g' | sort -u >> "${tmp_dir}/crtsh_output.txt"
      echo "Done!"

      if [ ${#dns_wordlists[@]} -gt 0 ]; then
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} We will execute gobuster and dnssearch ${#dns_wordlists[@]} time(s)."
          for list in "${dns_wordlists[@]}"; do
              index=$(printf "%s\n" "${dns_wordlists[@]}" | grep -En "^${list}$" | awk -F":" '{print $1}')
              if [ -s "${list}" ]; then
                  echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}... "
                  ${gobuster_bin} dns -z -q -r "${resolver_dns}" -t "${gobuster_threads}" \
                      -d "${domain}" -w "${list}" > "${tmp_dir}/gobuster_dns_output_${index}.txt" 2>&1
                  ${dnssearch_bin} -consumers 600 -domain "${domain}" \
                      -wordlist "${list}" > "${tmp_dir}/dnssearch_output_${index}.txt" 2>&1
                  echo "Done!"
              else
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}, error: ${list} does not exist or is empty!"
                  continue
              fi
              unset index
          done
          unset list
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution of gobuster and dnssearch is done."
      fi

  else
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script"
      exit 1
  fi
}

adjust_files(){
  if [ -d "${tmp_dir}" ] && [ -d "${report_dir}" ]; then
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Putting all domains only in one file and getting IPs block!!!"

      if [ -s "${tmp_dir}/amass_output.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass output... "
          grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_output.txt" | \
              grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
          grep -A 1000 "OWASP Amass.*OWASP/Amass" "${tmp_dir}/amass_output.txt" >> "${report_dir}/amass_blocks_output.txt"
          grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
              sed '/0.0.0.0\/0/d' >> "${report_dir}/ips_blocks.txt"
          echo "Done!"
      fi

      if [ -s "${tmp_dir}/amass_passive_output.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass passive output... "
          grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_passive_output.txt" | \
              grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
          echo "Done!"
      fi

      if [ -s "${tmp_dir}/sublist3r_output.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the sublist3r output... "
          sed -i -e 's/<BR>/\n/g' -e '1,10d' "${tmp_dir}/sublist3r_output.txt" 
          grep -Ev "Searching.*in|Starting.*subbrute|Total.*Found|Error:.*requests|Finished.*Enumeration|Warning:.*resolvers.txt"  \
              "${tmp_dir}/sublist3r_output.txt" | sort -u >> "${tmp_dir}/domains_tmp.txt"
          echo "Done!"
      fi

      if [ -s "${tmp_dir}/certspotter_output.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the certspotter output... " 
          sort -u "${tmp_dir}/certspotter_output.txt" >> "${tmp_dir}/domains_tmp.txt"
          echo "Done!"
      fi

      if [ -s "${tmp_dir}/crtsh_output.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the crtsh output... " 
          sort -u "${tmp_dir}/crtsh_output.txt" >> "${tmp_dir}/domains_tmp.txt"
          echo "Done!"
      fi
        
      if [ ${#dns_wordlists[@]} -gt 0 ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the gobuster dns output... "
          files=("${tmp_dir}"/gobuster_dns_output_*.txt)
          for file in "${files[@]}"; do
              if [ -s "${file}" ]; then
                  awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
              else
                  echo " "
                  echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                  continue
              fi
          done
          unset file
          unset files
          echo "Done!"

          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the dnsearch output... "
          files=("${tmp_dir}"/gobuster_dns_output_*.txt)
          for file in "${files[@]}"; do
              if [ -s "${file}" ]; then
                  awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
              else
                  echo " "
                  echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                  continue
              fi
          done
          unset file
          unset files
          echo "Done!"
      fi

      if [ -s "${tmp_dir}/domains_tmp.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Removing duplicated subdomains and unavailable domains... "
          if tr '[:upper:]' '[:lower:]' < "${tmp_dir}/domains_tmp.txt" | sed -e 's/^\.//' -e 's/^-//' | \
              sed -e 's/^http:\/\///' -e 's/^https:\/\///' | sort -u > "${report_dir}/domains_all.txt" ; then
              # Getting the domains that does not work
              while IFS= read -r subdomain; do
                  if host -t A "${subdomain}" | grep -E "NXDOMAIN|SERVFAIL" > /dev/null 2>&1 ; then
                      echo "${subdomain}" >> "${report_dir}/domains_null.txt"
                  fi
              done < "${report_dir}/domains_all.txt"
              unset subdomain
              # Remove domains that does not work and remove from domains_all.txt
              while IFS= read -r subdomain; do
                  if host -t A "${subdomain}" | grep -E "NXDOMAIN|SERVFAIL" > /dev/null 2>&1 ; then
                      sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"
                  fi
              done < "${report_dir}/domains_null.txt"
              unset subdomain
          else
              echo " "
              echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error removing duplicated subdomains and unavailable domains!"
              exit 1
          fi     
          echo "Done!"
      else
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The file with all domains from initial recon does not exist or is empty."
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Look all files from initial recon in ${tmp_dir} and fix the problem!"
          exit 1
      fi

      if [ ${#excluded[@]} -gt 0 ] && [ -s "${report_dir}/domains_all.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Excluding the subdomains from command line option... "
          for subdomain in "${excluded[@]}" ;do
              sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"
          done
          unset subdomain
          echo "Done!"
      fi

  else
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
      exit 1
  fi
}

diff_domains(){
  if [ -d "${report_dir}" ]; then 
      if [ -s "${report_dir}/domains_all.txt" ]; then
          data=$(date +%Y%m%d) ; (( data-=1 ))
          #oldest_domains_file=$(find ./${domain} -name domains_all.* -type f -printf '%T+ %p\n' | sort -u | head -n 1 | awk '{print $2}')
          oldest_domains_file=$(find "./${domain}" -name domains_all.txt -type f | sort -u | grep "${data}" | awk '{print $2}')
          if [[ -n "${oldest_domains_file}" ]]; then
              if cmp -s "${oldest_domains_file}" "${report_dir}/domains_all.txt"; then
                  echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting the difference between files to improve the time running recon.sh... "
                  diff -au "${oldest_domains_file}" "${report_dir}/domains_all.txt" | grep -E '^\+' | sed -e '/+++/d' -e 's/^+//' >> "${report_dir}/domains_diff.txt"
                  if [ -s "${report_dir}/domains_diff.txt" ]; then
                      if mv "${report_dir}/domains_all.txt" "${report_dir}/domains_all.${date_recon}"; then
                          cp "${report_dir}/domains_diff.txt" "${report_dir}/domains_all.txt"
                      else
                          echo " "
                          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error during move domains_all.txt to domains_all.old."
                          echo -e "\t Stopping the script!"
                          exit 1
                      fi
                  else
                      echo " "
                      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There isn't any changes since last execution."
                      echo -e "\t Stopping the script!"
                      exit 1
                  fi
                  echo "Done!"
              else
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The files are same since last execution of recon.sh script!"
                  echo -e "\t Stopping the script!"
                  exit 1
              fi
          else
              echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There isn't nothing in oldest_domains_file var, this is the first execution of recon.sh script!"
          fi
          unset data
      else
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} diff_domains function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
          exit 1
      fi
  else
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
      exit 1
  fi
}

nmap_ips(){
  if [ -d "${report_dir}" ]; then
      if [ -s "${report_dir}/domains_all.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting subdomains IP to use with nmap... "
          while IFS= read -r subdomain; do
              host -t A "${subdomain}" | grep -Ev "NXDOMAIN|SERVFAIL" | \
                  grep "has address" | sort | awk '{print $1"\t"$4}' >> "${report_dir}/domains_ips.txt"
              host -t A "${subdomain}" | grep -Ev "NXDOMAIN|SERVFAIL" | \
                  grep "alias" | sort | awk '{print $1"\t"$6}' | sed -e 's/\.$//'>> "${report_dir}/domains_aliases.txt"
          done < "${report_dir}/domains_all.txt"
          awk '{print $2}' "${report_dir}/domains_ips.txt" | sort -u >> "${report_dir}/nmap_ips.txt"
          unset subdomain
          echo "Done!"
      else
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} nmap_ips function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
          exit 1
      fi
   
      if [ -s "${report_dir}/ips_blocks.txt" ]; then
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting IPs from blocks with nmap -sn \"block\"..."
          count=0
          while IFS= read -r block; do
              block_file=nmap_$(echo "${block}" | sed -e 's/\//_/').txt
              cidr=$(echo "${block}" | awk -F'/' '{print $2}')
              if [[ ${cidr} -ge 21 ]]; then
                  nmap -sn "${block}" --exclude 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 --max-retries 3 --host-timeout 3 \
                      | grep -E "Nmap.*for" | awk '{print $6}' | sed -e 's/(//' -e 's/)//' > "${report_dir}"/"${block_file}"
                  sed -i '/^$/d' "${report_dir}/${block_file}"
                  (( count+=1 ))
              else
                  continue
              fi
              unset block_file
              unset cidr
          done < "${report_dir}/ips_blocks.txt"
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Got IP blocks, done!"
          if [[ ${count} -lt $(wc -l "${report_dir}/ips_blocks.txt" | awk '{print $1}') ]]; then
              echo -e "${red}Warning:${reset} Just ${count} block(s) were scanned, please look at ${report_dir}/ips_blocks.txt"
              echo -e "\t and nmap blocks files to know what were excluded blocks."
          fi
          unset count
      fi
  else
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
      exit 1
  fi
}

hosts_alive(){
  if [ -d "${report_dir}" ]; then
      if [ -s "${report_dir}/domains_all.txt" ]; then
          echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Testing subdomains to know if it is or have web application... "
          while IFS= read -r subdomain; do
              echo "${subdomain}" | ${httprobe_bin} \
                  -p http:8080 -p http:8443 -p http:8081 -p http:8010 -p http:8085 -p http:8086 \
                  -p http:8087 -p http:8008 | sort -u >> "${report_dir}/domains_web.txt" 
          done < "${report_dir}/domains_all.txt"
          unset subdomain
          echo "Done!"
          if cp "${report_dir}/domains_all.txt" "${report_dir}/domains_infra.txt"; then
              echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Separating infrastructure from web application... "
              while IFS= read -r line; do
                  subdomain=$(echo "${line}" | sed -e "s/http:\/\///" -e "s/https:\/\///" | awk -F":" '{print $1}')
                  if  grep "${subdomain}" "${report_dir}"/domains_infra.txt > /dev/null 2>&1 ; then
                      sed -i "/^${subdomain}$/d" "${report_dir}"/domains_infra.txt
                  else
                      continue
                  fi
                  unset subdomain
              done < "${report_dir}/domains_web.txt"
              echo "Done!"
          fi
          if [ -s "${report_dir}/domains_web.txt" ] && [ -s "${report_dir}/domains_infra.txt" ]; then
              echo -e "\t We have $(wc -l "${report_dir}/domains_web.txt" | awk '{print $1}') Web Applications URLs."
              echo -e "\t We have $(wc -l "${report_dir}/domains_infra.txt" | awk '{print $1}') Infrastructure domains."
          fi
      else
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} hosts_alive function error: the ${report_dir}/domains_all.txt does not exist or is empty."
          exit 1
      fi
  else
      echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
      exit 1
  fi
}

web_data(){
  if [ $# != 1 ]; then
      echo "Please, especify just 1 file to get URL from."
      exit 1
  else
      urls_file=$1
      if [ -s "${urls_file}" ]; then
          if [ -d "${report_dir}" ] && [ -d "${wayback_dir}" ] && [ -d "${web_data_dir}" ]; then
              if [ ${#web_wordlists[@]} -gt 0 ]; then
                  echo -e "${red}Warning:${reset} It can take a long time to execute!"
                  echo -e "\t We have ${#web_wordlists[@]} wordlists and $(wc -l "${urls_file}" | awk '{print $1}') urls to scan." 
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function will use ${#web_wordlists[@]} wordlists with gobuster and dirsearch... "
                  for list in "${web_wordlists[@]}"; do
                      index=$(printf "%s\n" "${web_wordlists[@]}" | grep -En "^""${list}""$" | awk -F":" '{print $1}')
                      if [ -s "${list}" ]; then
                          count=1
                          delay=1
                          proxy_port=8118
                          while IFS= read -r url; do
                              # Mounting the file names
                              name="$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")"
                              file_gobuster="gobuster_${name}_${index}.txt"
                              file_dirsearch="dirsearch_${name}_${index}.txt"
                                    
                              if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                                  name_instance="privoxy-${count}"
                                  # Running a docker proxy instance to try bypass some protection system like WAF
                                  ${docker_bin} run -d --name "${name_instance}" -p "${proxy_port}:8118" dperson/torproxy > /dev/null
                                  proxy_ip=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' "privoxy-${count}")

                                  # Skipping the specific wordlist from dirsearch on gobuster
                                  if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" &>/dev/null ; then
                                      ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                          -w "${list}" --proxy "${proxy_ip}:${proxy_port}" \
                                          -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                  else
                                      ${gobuster_bin} dir --delay 300ms -k -z -t "${gobuster_threads}" -x "${web_extensions}" -w "${list}" \
                                          -u "${url}" --proxy "${proxy_ip}:${proxy_port}" > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                      ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                          -w "${list}" --proxy "${proxy_ip}:${proxy_port}" \
                                          -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                  fi
                              else
                                  # Skipping the specific wordlist from dirsearch on gobuster
                                  if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" &>/dev/null ; then
                                      ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                          -w "${list}" --proxy "${proxy_ip}:${proxy_port}" \
                                          -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                  else
                                      ${gobuster_bin} dir --delay 300ms -k -z -t "${gobuster_threads}" -x "${web_extensions}" -w "${list}" \
                                          -u "${url}" --proxy "${proxy_ip}:${proxy_port}" > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                      ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                          -w "${list}" --proxy "${proxy_ip}:${proxy_port}" \
                                          -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                  fi
                              fi
                              sleep "${delay}"
                              (( count+=1 ))
                              (( delay+=2 ))
                              (( proxy_port+=1 ))
                              unset file_dirsearch
                              unset file_gobuster
                              unset name
                              unset url
                              unset name_instance
                          done < "${urls_file}"
                          unset count
                          unset delay
                          unset proxy_port
                          unset proxy_ip
                      else
                          echo " "
                          echo -e "\t ${red}Error:${reset} ${list} does not exist or is empty!"
                          continue
                      fi
                      unset delay
                      unset index
                      unset list
                  done
                  if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                      echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Stopping the privoxy instances... "
                      docker stop $(docker ps | grep privoxy | awk '{print $1}') > /dev/null
                      docker rm $(docker ps -aq) > /dev/null
                      echo "Done!"
                  fi
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function is done!"
              else
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} dirseach/goboster web_data function error: array of wordlists is empty. Stopping the script"
                  exit 1
              fi
              echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing wayback... "
              while IFS= read -r url; do
                  name=$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")
                  file="wayback_${name}.txt"
                  echo "${url}" | ${wayback_bin} > "${wayback_dir}/${file}" 2>&1
                  unset file
              done < "${urls_file}"
              unset url
              echo "Done!"
          else
              echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
              unset urls_file
              exit 1
          fi
      else
          echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the ${urls_file} exist and isn't empty. Stopping the script."
          unset urls_file
      fi
      unset urls_file
  fi
}

cleanup_web_data_files(){
  echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up dirsearch files... "
  files=("${web_data_dir}"/dirsearch*.txt)
  for file in "${files[@]}"; do
      sed -i -e 's/.\[4.m//g' -e 's/.\[3.m//g' -e 's/.\[1K.\[0G/\n/g' "${file}" 2> /dev/null
      sed -i -e 's/.\[1m//g' -e 's/.\[0m//g' "${file}" 2> /dev/null
      sed -i -e '/Last request to/d' "${file}" 2> /dev/null
      #sed -i -e '/^$/d' "${file}" 2> /dev/null
  done 
  echo "Done!"
  unset file
  unset files
       
  echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up gobuster files... "
  files=("${web_data_dir}"/gobuster*.txt)
  for file in "${files[@]}"; do
      sed -i "s/^..\[2K//" "${file}" 2> /dev/null
  done 
  echo "Done!"
  unset file
  unset files
}

robots_txt(){
  echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for new URLs on robots.txt... "
  files=("${web_data_dir}"/*.txt)
  for file in "${files[@]}"; do
      if grep robots.txt "${file}" > /dev/null && [ -s "${file}" ] ; then
          target=$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//') 
          for url in $(curl -s "${target}"/robots.txt | grep -Ev "User-agent: *" | awk '{print $2}' | sed -e "/^\/$/d"); do
              echo "${target}${url}" >> "${report_dir}/robots_urls.txt"
              sed -i -e 's/\r//g' -e 's/\/$//g' "${report_dir}/robots_urls.txt"
          done
      fi
      unset target
      unset file
  done 
  echo "Done!"
  unset files
}

aquatone_function(){
  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Starting aquatone scan... "
  for file in "${report_dir}/domains_web.txt" "${report_dir}/robots_urls.txt"; do
      if [ -s "${file}" ]; then
          aquatone_log=${tmp_dir}/aquatone_$(basename "${file}" | awk -F'.' '{print $1}').log
          aquatone_files_dir=${aquatone_data}/$(basename "${file}" | awk -F'.' '{print $1}')
          if [ ! -d "${aquatone_files_dir}" ]; then
              if mkdir -p "${aquatone_files_dir}" ; then
                  "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
              else
                  echo " "
                  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Something got wrong, wasnt possible create directory ${aquatone_files_dir}."
                  echo -e "\t Please, look what got wrong and run the script again. Stopping the script!"
                  exit 1
              fi
          else
              "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
          fi
          sleep 60
      else
          echo -e "\t The ${file} does not exist or is empty!"
      fi
  done
  unset aquatone_log
  unset aquatone_files_dir
  unset file
  echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Finish aquatone scan!"
}

#report(){
#   Falta criar a parte de report, página web, subir o server em python, etc...
#}

clean_up(){
  echo -n "Cleanning empty files... "
  echo "Done!"
  echo -n "Cleanning vars... "
  unset tmp_dir
  unset report_dir
  unset wayback_dir
  unset aquatone_data
  unset recon_dir
  unset date_recon
  unset excluded
  unset domain
  unset web_data_dir
  echo "Done!"
}

# Initiating the recon.sh script
(
subdomains_recon
adjust_files
diff_domains
nmap_ips
hosts_alive
web_data "${report_dir}/domains_web.txt"
cleanup_web_data_files
robots_txt
web_data "${report_dir}/robots_urls.txt"
cleanup_web_data_files
aquatone_function
#report
#clean_up
message
) 2>&1 | tee -a "${execution_log}"
