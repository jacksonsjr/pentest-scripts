#!/bin/bash
# my recon.sh

# Colours
red="\e[31m"
green="\e[32m"
yellow="\e[33m"
reset="\e[0m"

if [[ -s ./recon.config ]]; then
    source ./recon.config
else
    echo -e "Please ${red}make sure${reset} you have the ${yellow}recon.config${reset} file."
    echo -e "${yellow}You need this to set the tools parameters${reset}!"
    echo -e "You will set aquatone, gobuster and dirsearch threads, docker network for privoxy instances and others."
    exit 1
fi

# Get date
date_recon=$(date +%Y%m%d)
# List of directories
pentest_dir=${HOME}/pentest
tools_dir=${pentest_dir}/tools
#wordlists_dir=${pentest_dir}/wordlists

# Verifying if all binaries there are in the system
count=0
for binary in aquatone amass diff dig dirsearch.py dnssearch docker gobuster \
   host httprobe jq nmap subfinder sublist3r.py waybackurls whois ; do
   if ! command -v "${binary}" > /dev/null 2>&1 ; then
      echo -e "The ${red}${binary} does not exist${reset} on the system!"
      ((count+=1))
   fi
done
if [ "${count}" -gt 0 ]; then
   echo -e "Please, ${yellow}make sure${reset} you got all tools (binaries and scripts)."
   echo -e "You could use the ${yellow}get-tools.sh${reset} to get all binaries and scripts!"
   echo -e "Or you could use the recon.sh ${yellow}inside docker${reset} with Dockerfile in current dir."
   unset count
   exit 1
fi

# List of 3rd party binaries and python scripts to use in this script
amass_bin=$(command -v amass)
aquatone_bin=$(command -v aquatone)
dirsearch_bin=$(command -v dirsearch.py)
dnssearch_bin=$(command -v dnssearch)
docker_bin=$(command -v docker)
gobuster_bin=$(command -v gobuster)
httprobe_bin=$(command -v httprobe)
subfinder_bin=$(command -v subfinder)
sublist3r_bin=$(command -v sublist3r.py)
wayback_bin=$(command -v waybackurls)

# Get the correct path to use chromium with aquatone
for binary in /usr/bin/chromium /usr/bin/chromium-browser; do
    if command -v "${binary}" > /dev/null 2>&1 ; then
       chromium_bin=/usr/bin/chromium
    elif command -v "${binary}" > /dev/null 2>&1 ; then
       chromium_bin=/usr/bin/chromium-browser
    fi
done

# List of wordlists to use in this script
web_wordlists=("${tools_dir}/dirsearch/db/dicc.txt")
dns_wordlists=()

if [ ${#web_wordlists[@]} -eq 0 ]; then
    echo -e "Please, ${yellow}make sure${reset} you have the default wordlists!"
    exit 1
fi

# Script usage description
usage(){
    (
    echo -e "Usage: ${yellow}$0${reset} ${green}-d domain.com${reset}"
    echo "Options: "
    echo -e "\t-b|--brute-sublist3r  - if specified the Sublist3r will do brute force, this option take a long time to finish"
    echo -e "\t\t\t\t${yellow}use -b or --brute-sublist3r${reset}"
    echo -e "\t-d|--domain           - specify a valid domain [${red}needed${reset}]"
    echo -e "\t-e|--exclude-domains  - specify excluded subdomains after all treated files"
    echo -e "\t\t\t\t${yellow}use -e domain1.com,domain2.com or --exclude-domains domain1.com,domain2.com${reset}"
    echo -e "\t-l|--limit-urls       - specify the url quantity to run dirsearch and gobuster for dirs and files enumeration"
    echo -e "\t\t\t\t${yellow}use -l 10 or --limit-urls 10${reset}"
    echo -e "\t-p|--proxy            - this option when specified will use privoxy instance using docker trying to avoid or bypass WAF block"
    echo -e "\t\t\t\t${yellow}use -p or --proxy${reset}"
    echo -e "\t-r|--resolver         - specify the DNS to resolve, default value is 8.8.8.8"
    echo -e "\t\t\t\t${yellow}use -r 1.1.1.1 or --resolver 8.8.8.8${reset}"
    echo -e "\t-s|--subdomain-brute  - specify the wordlist to put in dns_wordlist array and execute gobuster and dnssearch brute force"
    echo -e "\t\t\t\tthis option take a long time to finish, use this own your need, by default the array is empty"
    echo -e "\t\t\t\tand not execute gobuster and dnssearch. The success of use those tools is a good wordlist."
    echo -e "\t\t\t\t${yellow}use -s /path/to/wordlist1,/path/to/wordlist2 or --subdomain-brute /path/to/wordlist1,/path/to/wordlist2${reset}"
    echo -e "\t-w|--web-wordlists    - specity more wordlists to put in web_wordlist array, by default we use the"
    echo -e "\t\t\t\t${tools_dir}/dirsearch/db/dicc.txt as the first wordlist"
    echo -e "\t\t\t\tto enumerate dirs and files from website."
    echo -e "\t\t\t\t${yellow}use -w /path/to/wordlist1,/path/to/wordlist2 or --web-wordlists /path/to/wordlist1,/path/to/wordlist2${reset}"
    ) 1>&2; exit 1
}

options+=(-b --brute-sublist3r -d --domain -e --exclude-domains)
options+=(-l --limit-urls -p --proxy -r --resolver)
options+=(-s --subdomain-brute -w --web-wordlists)

while :; do
    case $1 in
        -b|--brute-sublist3r)
            brute_sublist3r=yes
            shift
            ;;
        -d|--domain)
            if [ "$2" ] && [[ $(host -t A "$2" > /dev/null 2>&1; echo $?) -eq 0 ]]; then
                domain="$2"
                shift 2
            else
                usage
            fi
            ;;
        -e|--exclude-domains)
            set -f
            IFS=","
            excluded+=($2)
	        unset IFS
            for subdomain in "${excluded[@]}"; do
                for option in "${options[@]}"; do
                   [[ "${subdomain}" == "${option}" ]] && usage 
               done
            done
            shift 2
            ;;
        -l|--limit-urls)
            if [[ $2 == ?(-)+([0-9]) ]]; then
                limit_urls="$2"
                shift 2
            else
                usage
            fi
            ;;
        -p|--proxy)
            use_proxy=yes
            shift
            ;;
        -r|--resolver)
            if [[ $2 =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
                unset resolver_dns
                resolver_dns="$2"
                shift 2
            else
                usage
            fi
            ;;
        -s|--subdomain-brute)
            set -f
            IFS=","
            dns_wordlists+=($2)
            unset IFS
            for wordlist in "${dns_wordlists[@]}"; do
                for option in "${options[@]}"; do
                   [[ "${wordlist}" == "${option}" ]] && usage 
               done
            done
            shift 2
            ;;
        -w|--web-wordlists)
            set -f
            IFS=","
            web_wordlists+=($2)
            unset IFS
            for wordlist in "${web_wordlists[@]}"; do
                for option in "${options[@]}"; do
                   [[ "${wordlist}" == "${option}" ]] && usage 
               done
            done
            shift 2
            ;;
        -?*)
            usage
            ;;
        *)
            break
    esac
done

[[ -z ${domain} ]] && usage

# Create all dirs necessaries to report and recon 
if [ -d "./${domain}" ]; then
    echo "This is a known target." 
fi

echo -n "Preparing the directories structure to work... "
mkdir -p ./"${domain}"/{log,recon_"${date_recon}"}
log_dir="./${domain}/log"
recon_dir="./${domain}/recon_${date_recon}"
mkdir -p "${recon_dir}"/{tmp,report,wayback-data,aquatone,web-data}
tmp_dir="${recon_dir}/tmp"
report_dir="${recon_dir}/report"
wayback_dir="${recon_dir}/wayback-data"
aquatone_data="${recon_dir}/aquatone"
web_data_dir="${recon_dir}/web-data"
echo "Done!"

message() {
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} ${green}Recon finished on${reset} ${yellow}${domain}${reset}${green}!${reset}"
    echo -e "\t ${green}Consider to use recon-ng and theHarvester to help get more assets!${reset}"
    echo -e "\t ${green}Use Shoda.io, Censys and others.${reset}"
}

subdomains_recon(){
    if [ -d "${tmp_dir}" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} ${green}Recon started on${reset} ${yellow}${domain}${reset}${green}!${reset}"
        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing amass... "
        ${amass_bin} enum -d "${domain}" > "${tmp_dir}/amass_output.txt" 2>&1
        #${amass_bin} enum --passive -d "${domain}" > "${tmp_dir}/amass_passive_output.txt" 2>&1
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing certspotter... "
        curl -s https://certspotter.com/api/v0/certs\?domain="${domain}" | jq '.[].dns_names[]' | \
            sed 's/\"//g' | sed 's/\*\.//g' | sort -u | grep "${domain}" >> "${tmp_dir}/certspotter_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing crt.sh... "
        curl -s https://crt.sh/?q=%."${domain}"\&output=json | jq -r '.[].name_value' | \
            sed 's/\*\.//g' | sort -u >> "${tmp_dir}/crtsh_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing dns bufferover... "
        curl -s https://dns.bufferover.run/dns?q="${domain}" | jq -r '.FDNS_A[]' \
            | sed -e "s/\"//" -e "s/\",$//" -e "s/\"//" -e "s/\t\t//" \
            | grep -E "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b" | awk -F"," '{print $2}' \
            | sort -u >> "${tmp_dir}/dnsbufferover_output.txt"
        echo "Done!"

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing subfinder... "
        ${subfinder_bin} -silent -d "${domain}" > "${tmp_dir}/subfinder_output.txt"
        echo "Done!"
        
        if [ -n "${brute_sublist3r}" ] && [ "${brute_sublist3r}" == "yes" ]; then
            echo -e "${red}Warning:${reset} Sublist3r take almost 30 minutes to be executed!"
            echo -e "\t If you find yourself taking longer than expected: "
            echo -e "\t ${yellow}>>${reset} check for a zombie python process;"
            echo -e "\t ${yellow}>>${reset} stop the script;"
            echo -e "\t ${yellow}>>${reset} change the default parameters of the Sublist3r in the script to a lower value;"
            echo -e "\t ${yellow}>>${reset} execute the script again;"
            echo -e "\t ${yellow}>>${reset} if the problem persists, check your internet connection."
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
            "${sublist3r_bin}" -n -d "${domain}" -b -t "${sublist3r_threads}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
            echo "Done!"
        else
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing Sublist3r... "
            ${sublist3r_bin} -n -d "${domain}" > "${tmp_dir}/sublist3r_output.txt" 2>&1
            echo "Done!"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} We will execute gobuster and dnssearch ${#dns_wordlists[@]} time(s)."
            for list in "${dns_wordlists[@]}"; do
                index=$(printf "%s\n" "${dns_wordlists[@]}" | grep -En "^${list}$" | awk -F":" '{print $1}')
                if [ -s "${list}" ]; then
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}... "
                    ${gobuster_bin} dns -z -q -r "${resolver_dns}" -t "${gobuster_threads}" \
                        -d "${domain}" -w "${list}" > "${tmp_dir}/gobuster_dns_output_${index}.txt" 2>&1
                    ${dnssearch_bin} -consumers 600 -domain "${domain}" \
                        -wordlist "${list}" > "${tmp_dir}/dnssearch_output_${index}.txt" 2>&1
                    echo "Done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution number ${index}, error: ${list} does not exist or is empty!"
                    continue
                fi
                unset index
            done
            unset list
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Execution of gobuster and dnssearch is done."
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script"
        exit 1
    fi
    
    for ns in $(dig ns "${domain}" 2> /dev/null | grep "^${domain}\." | awk '{print $5}' | sed -e 's/\.$//'); do
        if ! dig axfr "@${ns}" "${domain}" 2> /dev/null | \
            grep -Ei "Transfer failed.|servers could be reached|timed out." > /dev/null 2>&1; then
            dig axfr "@${ns}" "${domain}" >> "${tmp_dir}/zone_transfer.txt"
        fi
    done
}

adjust_files(){
    if [ -d "${tmp_dir}" ] && [ -d "${report_dir}" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Putting all domains only in one file and getting IPs block!!!"

        if [ -s "${tmp_dir}/amass_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass output... "
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_output.txt" | \
                grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            grep -A 1000 "OWASP Amass.*OWASP/Amass" "${tmp_dir}/amass_output.txt" >> "${report_dir}/amass_blocks_output.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -vE "^([0-9a-zA-Z]{1,4}:)" | sed '/0.0.0.0\/0/d' >> "${report_dir}/ips_blocks.txt"
            grep "Subdomain Name(s)" "${report_dir}/amass_blocks_output.txt" | awk '{print $1}' | \
                grep -E "^([0-9a-zA-Z]{1,4}:)" >> "${report_dir}/ipv6_blocks.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/amass_passive_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the amass passive output... "
            grep -Ev "Starting.*names|Querying.*|Average.*performed" "${tmp_dir}/amass_passive_output.txt" | \
                grep "${domain}" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/certspotter_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the certspotter output... " 
            sort -u "${tmp_dir}/certspotter_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/crtsh_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the crtsh output... " 
            sort -u "${tmp_dir}/crtsh_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi
        
        if [ -s "${tmp_dir}/dnsbufferover_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the dns bufferover output... " 
            sort -u "${tmp_dir}/dnsbufferover_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/subfinder_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the subfinder output... " 
            sort -u "${tmp_dir}/subfinder_output.txt" >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ -s "${tmp_dir}/sublist3r_output.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the sublist3r output... "
            sed -i -e 's/<BR>/\n/g' -e '1,10d' "${tmp_dir}/sublist3r_output.txt" 
            grep -Ev "Searching.*in|Starting.*subbrute|Total.*Found|Error:.*requests|Finished.*Enumeration|Warning:.*resolvers.txt"  \
                "${tmp_dir}/sublist3r_output.txt" | sort -u >> "${tmp_dir}/domains_tmp.txt"
            echo "Done!"
        fi

        if [ ${#dns_wordlists[@]} -gt 0 ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the gobuster dns output... "
            files=("${tmp_dir}"/gobuster_dns_output_*.txt)
            for file in "${files[@]}"; do
                if [ -s "${file}" ]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
                    echo "Done!"
                else
                    echo " "
                    echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                    continue
                fi
            done
            unset file
            unset files

            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleanning up the dnsearch output... "
            files=("${tmp_dir}"/gobuster_dns_output_*.txt)
            for file in "${files[@]}"; do
                if [ -s "${file}" ]; then
                    awk '{print $2}' "${file}" | tr '[:upper:]' '[:lower:]' | sort -u >> "${tmp_dir}/domains_tmp.txt"
                    echo "Done!"
                else
                    echo " "
                    echo -e "\t ${red}Error${reset}: file does not exist or is empty!"
                    continue
                fi
            done
            unset file
            unset files
        fi

        if [ -s "${tmp_dir}/domains_tmp.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains... "
            # Removing duplicated subdomains
            if tr '[:upper:]' '[:lower:]' < "${tmp_dir}/domains_tmp.txt" | sed -e 's/^\.//' -e 's/^-//' -e 's/\.\./\./' | \
                sed -e 's/^http:\/\///' -e 's/^https:\/\///' -e 's/ //' | sort -u > "${report_dir}/domains_all.txt" ; then
                # Domains and subdomains resolution
                while IFS= read -r subdomain; do
                    host -t A "${subdomain}" >> "${tmp_dir}/domains_resolution.txt"
                done < "${report_dir}/domains_all.txt"
                unset subdomain
                # Sorting out...
                if [ -s "${tmp_dir}/domains_resolution.txt" ] ; then
                    sed -i -e 's/\.$//' "${tmp_dir}/domains_resolution.txt"
                    # Domains with IPs
                    grep "has address" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $1"\t"$4}' >> "${report_dir}/domains_ips.txt"
                    # Domains aliases
                    grep "alias" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $1"\t"$6}' >> "${report_dir}/domains_aliases.txt"
                    # Unavailable domains
                    grep -E "NXDOMAIN|SERVFAIL" "${tmp_dir}/domains_resolution.txt" | sort -u | awk '{print $2}' >> "${report_dir}/domains_null.txt"
                else
                    echo " " 
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Something got wrong on domains_all.txt resolution, please fix it."
                    exit 1
                fi
                # Remove domains that does not work from domains_all.txt
                if [ -s "${report_dir}/domains_null.txt" ]; then
                    while IFS= read -r subdomain; do
                        sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"  
                    done < "${report_dir}/domains_null.txt"
                    unset subdomain
                fi
            else
                echo " "
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error sorting out domains, subdomains, IPs, duplicated subdomains and unavailable domains!"
                exit 1
            fi     
            echo "Done!"
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The file with all domains from initial recon does not exist or is empty."
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Look all files from initial recon in ${tmp_dir} and fix the problem!"
            exit 1
        fi

        if [ ${#excluded[@]} -gt 0 ] && [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Excluding the subdomains from command line option... "
            for subdomain in "${excluded[@]}" ;do
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_all.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_ips.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_aliases.txt"
                sed -i "/^${subdomain}$/d" "${report_dir}/domains_null.txt"
            done
            unset subdomain
            echo "Done!"
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

diff_domains(){
    if [ -d "${report_dir}" ]; then 
        if [ -s "${report_dir}/domains_all.txt" ]; then
            oldest_domains_all=$(find "./${domain}" -name domains_all.txt -type f | sort -u | grep -v "${date_recon}" | tail -n1)
            oldest_domains_web=$(find "./${domain}" -name domains_web.txt -type f | sort -u | grep -v "${date_recon}" | tail -n1)
            if [[ -n "${oldest_domains_all}" ]] && [[ -n "${oldest_domains_web}" ]]; then
                if cmp -s "${oldest_domains_all}" "${report_dir}/domains_all.txt"; then
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting the difference between files to improve the time running recon.sh... "
                    diff -au "${oldest_domains_all}" "${report_dir}/domains_all.txt" | grep -E '^\+' | sed -e '/+++/d' -e 's/^+//' >> "${report_dir}/domains_diff.txt"
                    if [ -s "${report_dir}/domains_diff.txt" ]; then
                        if mv "${report_dir}/domains_all.txt" "${report_dir}/domains_all.${date_recon}"; then
                            cp "${report_dir}/domains_diff.txt" "${report_dir}/domains_all.txt"
                        else
                            echo " "
                            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Error during move domains_all.txt to domains_all.old."
                            echo -e "\t Stopping the script!"
                            exit 1
                        fi
                    else
                        echo " "
                        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There isn't any changes since last execution."
                        echo -e "\t Stopping the script!"
                        exit 1
                    fi
                    echo "Done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The files are same since last execution of recon.sh script!"
                    echo -e "\t Stopping the script!"
                    exit 1
                fi
            else
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} There is nothing different between, this execution and the last one!"
            fi
            unset data
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} diff_domains function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
            exit 1
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

nmap_ips(){
    if [ -d "${report_dir}" ]; then
        if [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting subdomains IP to use with nmap... "
            awk '{print $2}' "${report_dir}/domains_ips.txt" | sort -u >> "${report_dir}/nmap_ips.txt"
            echo "Done!"
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} nmap_ips function error: file ${report_dir}/domains_all.txt does not exist or is empty!"
            exit 1
        fi

        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting target ip blocks to use with nmap... "
        while IFS= read -r ip; do
            if whois "${ip}" 2> /dev/null | grep -q "${domain}"; then
                sleep 5
                whois "${ip}" 2> /dev/null | grep -E "^inetnum:|^inetrev:" | awk '{print $2}' 
            fi
        done < "${report_dir}/nmap_ips.txt" | sort -u >> "${tmp_dir}/blocks_owner_tmp.txt"
        unset ip

        while IFS= read -r subdomain; do
            if whois "${subdomain}" 2> /dev/null | grep -q "${domain}"; then
                sleep 5
                whois "${subdomain}" 2> /dev/null | grep -E "^inetnum:|^inetrev:" | awk '{print $2}' 
            fi
        done < "${report_dir}/domains_all.txt" | sort -u >> "${tmp_dir}/blocks_owner_tmp.txt"
        unset subdomain

        while IFS= read -r block; do
            if whois "${block}" 2> /dev/null | grep -q "${domain}"; then
                sleep 5
                whois "${block}" 2> /dev/null | grep -E "^inetnum:|^inetrev:" | awk '{print $2}'
            fi
        done < "${report_dir}/ips_blocks.txt" | sort -u >> "${tmp_dir}/blocks_owner_tmp.txt"
        unset block

        while IFS= read -r block; do
            if whois "${block}" 2> /dev/null | grep -q "${domain}"; then
                sleep 5
                whois "${block}" 2> /dev/null | grep -E "^inetnum:|^inetrev:" | awk '{print $2}'
            fi
        done < "${tmp_dir}/blocks_owner_tmp.txt" | sort -u >> "${tmp_dir}/blocks_tmp.txt"
        unset block

        if [[ -f "${tmp_dir}/blocks_owner_tmp.txt" ]] && [[ -f "${tmp_dir}/blocks_tmp.txt" ]]; then
            cat "${tmp_dir}/blocks_owner_tmp.txt" "${tmp_dir}/blocks_owner_tmp.txt" | sort -u > "${report_dir}/blocks_owner.txt"
        fi
        echo "Done!"

        if [ -s "${report_dir}/blocks_owner.txt" ]; then
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Getting IPs from blocks with nmap -sn \"block\"..."
            count=0
            while IFS= read -r block; do
                block_file=nmap_$(echo "${block}" | sed -e 's/\//_/').txt
                cidr=$(echo "${block}" | awk -F'/' '{print $2}')
                if [[ ${cidr} -ge 24 ]]; then
                    nmap -sn "${block}" --exclude 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 --max-retries 3 --host-timeout 3 \
                        | grep -E "Nmap.*for" | awk '{print $6}' | sed -e 's/(//' -e 's/)//' > "${report_dir}"/"${block_file}" 2> /dev/null
                    sed -i '/^$/d' "${report_dir}/${block_file}"
                    (( count+=1 ))
                else
                    continue
                fi
                unset block_file
                unset cidr
            done < "${report_dir}/blocks_owner.txt"
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Got IP blocks, done!"
            if [[ ${count} -lt $(wc -l "${report_dir}/blocks_owner.txt" | awk '{print $1}') ]]; then
                echo -e "${red}Warning:${reset} Just ${count} block(s) were scanned, please look at ${report_dir}/blocks_owner.txt"
                echo -e "\t and nmap blocks files to know what were excluded blocks."
            fi
            unset count
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

hosts_alive(){
    if [ -d "${report_dir}" ]; then
        if [ -s "${report_dir}/domains_all.txt" ]; then
            echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Testing subdomains to know if it is or have web application... "
            while IFS= read -r subdomain; do
                echo "${subdomain}" | ${httprobe_bin} -t "${httprobe_timeout}" \
                    -p http:8080 -p http:8443 -p http:8081 -p http:8010 -p http:8085 -p http:8086 \
                    -p http:8087 -p http:8008 | sort -u >> "${report_dir}/domains_web.txt" 
            done < "${report_dir}/domains_all.txt"
            unset subdomain
            echo "Done!"

            if cp "${report_dir}/domains_all.txt" "${report_dir}/domains_infra.txt"; then
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Separating infrastructure from web application... "
                while IFS= read -r line; do
                    subdomain=$(echo "${line}" | sed -e "s/http:\/\///" -e "s/https:\/\///" | awk -F":" '{print $1}')
                    if grep "${subdomain}" "${report_dir}"/domains_infra.txt > /dev/null 2>&1 ; then
                        sed -i "/^${subdomain}$/d" "${report_dir}"/domains_infra.txt  
                    else
                        continue
                    fi
                unset subdomain
                done < "${report_dir}/domains_web.txt"
                echo "Done!"
            fi
            
            if [ -s "${report_dir}/domains_web.txt" ] && [ -s "${report_dir}/domains_infra.txt" ]; then
                echo -e "\t We have $(wc -l "${report_dir}/domains_web.txt" | awk '{print $1}') Web Applications URLs."
                echo -e "\t We have $(wc -l "${report_dir}/domains_infra.txt" | awk '{print $1}') Infrastructure domains."
            fi

        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} hosts_alive function error: the ${report_dir}/domains_all.txt does not exist or is empty."
            exit 1
        fi
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
        exit 1
    fi
}

web_data(){
    if [ $# != 1 ]; then
        echo "Please, especify just 1 file to get URL from."
        exit 1
    else
        urls_file=$1
        if [ -s "${urls_file}" ]; then
            if [ -d "${report_dir}" ] && [ -d "${wayback_dir}" ] && [ -d "${web_data_dir}" ]; then
                if [ ${#web_wordlists[@]} -gt 0 ]; then
                    echo -e "${red}Warning:${reset} It can take a long time to execute!"
                    echo -e "\t We have ${#web_wordlists[@]} wordlists and $(wc -l "${urls_file}" | awk '{print $1}') urls to scan." 
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function will use ${#web_wordlists[@]} wordlists with gobuster and dirsearch... "
                    count=2
                    ip_instance=2
                    proxy_port=8118
                    third_oct=0
                    for list in "${web_wordlists[@]}"; do
                        index=$(printf "%s\n" "${web_wordlists[@]}" | grep -En "^""${list}""$" | awk -F":" '{print $1}')
                        urls_tested=1
                        if [ -s "${list}" ]; then
                            while IFS= read -r url; do
                                # Mounting the file names
                                name="$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")"
                                file_gobuster="gobuster_${name}_${index}.txt"
                                file_dirsearch="dirsearch_${name}_${index}.txt"
                                if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                                    [[ -z $(${docker_bin} network ls | grep ProxyNetwork | awk '{print $2}') ]] && \
                                       ${docker_bin} network create --driver bridge --subnet "${docker_network}.0.0/16" \
                                       --gateway "${docker_network}.0.1" ProxyNetwork > /dev/null
                                    name_instance="privoxy-${count}"
                                    if [[ $(${docker_bin} ps --format '{{.Names}}' | grep "${name_instance}") ]]; then
                                        unset name_instance
                                        (( count+=1 ))
                                        name_instance="privoxy-${count}"
                                    fi
                                    # Running a docker proxy instance to try bypass some protection system like WAF
                                    ${docker_bin} run -d --rm --name "${name_instance}" -p "${proxy_port}:8118" \
                                        --network ProxyNetwork --ip "${docker_network}.${third_oct}.${ip_instance}" privoxy > /dev/null
                                    proxy_ip="${docker_network}.${third_oct}.${ip_instance}"
                                    while [ -z "${Congratulations}" ]; do
                                        Congratulations=$(curl -s --proxy "${proxy_ip}:8118" 'https://check.torproject.org/' 2> /dev/null | grep -m1 "Congratulations" | awk '{print $1}')
                                    done                                    
                                    # Skipping the specific wordlist from dirsearch on gobuster
                                    if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" > /dev/null 2>&1 ; then
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" --proxy "${proxy_ip}:8118" \
                                            -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    else
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" --proxy "${proxy_ip}:8118" \
                                            -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                        ${gobuster_bin} dir -z -t "${gobuster_threads}" -x "${web_extensions}" -u "${url}" \
                                            --proxy "http://${proxy_ip}:8118" -k -w "${list}" --timeout 20s \
                                            > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                    fi
                                    while [[ "$(pgrep -acf "[d]irsearch|[g]obuster")" -ge "${webdata_total_processes}" ]]; do
                                        sleep 1
                                    done
                                    if [[ "$(pgrep -acf "[d]ocker-proxy")" -gt "$(pgrep -acf "[d]irsearch|[g]obuster")" ]]; then
                                        for instance in $(${docker_bin} ps --format '{{.Names}}' | grep "privoxy-"); do
                                            ip="$(${docker_bin} inspect --format '{{ .NetworkSettings.Networks.ProxyNetwork.IPAddress }}' "${instance}")"
                                            [[ ! $(pgrep -fa "[d]irsearch.py|[g]obuster" | awk '{print $12}' | sed -e 's/http:\/\///' | cut -d":" -f1 | sort -u | grep "${ip}") ]] && \
                                            ${docker_bin} stop "${instance}" > /dev/null 2>&1
                                        done
                                    fi
                                    if [[ ${ip_instance} -eq 254 ]]; then
                                        (( third_oct+=1 ))
                                        ip_instance=0
                                    fi
                                    (( ip_instance+=1 ))
                                else
                                    # Skipping the specific wordlist from dirsearch on gobuster
                                    if grep -E "\.\%EXT\%|\.\%EX\%" "${list}" > /dev/null 2>&1 ; then
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                    else
                                        ${dirsearch_bin} -t "${dirsearch_threads}" -e "${web_extensions}" --random-user-agents \
                                            -w "${list}" -u "${url}" > "${web_data_dir}/${file_dirsearch}" 2> /dev/null &
                                        ${gobuster_bin} dir --delay 300ms -k -z -t "${gobuster_threads}" -x "${web_extensions}" -w "${list}" \
                                            -u "${url}" > "${web_data_dir}/${file_gobuster}" 2> /dev/null &
                                    fi
                                    while [[ "$(pgrep -acf "[d]irsearch|[g]obuster")" -ge "${webdata_total_processes}" ]]; do
                                        sleep 1
                                    done
                                fi
                                (( count+=1 ))
                                (( proxy_port+=1 ))
                                [[ "${limit_urls}" -eq "${urls_tested}" ]] && break
                                (( urls_tested+=1 ))
                                unset file_dirsearch
                                unset file_gobuster
                                unset name
                                unset url
                            done < "${urls_file}"
                        else
                            echo " "
                            echo -e "\t ${red}Error:${reset} ${list} does not exist or is empty!"
                            continue
                        fi
                        unset index
                        unset list
                        unset urls_tested
                    done
                    unset count
                    unset ip_instance
                    unset proxy_port
                    unset third_oct
                    unset proxy_ip
                    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Waiting the dirsearch and/or gobuster finish... "
                    while pgrep -f dirsearch > /dev/null || pgrep -f gobuster > /dev/null; do
                        sleep 1
                    done
                    echo "Done!"
                    if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Stopping the privoxy instances... "
                        for instance in $(docker ps --format '{{.Names}}' | grep "privoxy-"); do
                            ${docker_bin} stop "${instance}" > /dev/null 2>&1
                        done
                        echo "Done!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Stopping the privoxy network on docker... "
                        ${docker_bin} network rm ProxyNetwork > /dev/null 2>&1
                        echo "Done!"
                    fi
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Web data function is done!"
                else
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} dirseach/goboster web_data function error: array of wordlists is empty. Stopping the script"
                    exit 1
                fi
                echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Executing wayback... "
                while IFS= read -r url; do
                    name=$(echo "${url}" | sed -e "s/http:\/\//http_/" -e "s/https:\/\//https_/" -e "s/:/_/" -e "s/\/$//" -e "s/\//_/g")
                    file="wayback_${name}.txt"
                    echo "${url}" | ${wayback_bin} > "${wayback_dir}/${file}" 2>&1
                    unset file
                done < "${urls_file}"
                unset url
                echo "Done!"
            else
                echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the directories structure was created. Stopping the script."
                unset urls_file
                exit 1
            fi
        else
            echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Make sure the ${urls_file} exist and isn't empty."
            unset urls_file
        fi
        unset urls_file
    fi
}

cleanup_web_data_files(){
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up dirsearch files... "
    for file in $(ls -1 "${web_data_dir}" | grep dirsearch); do 
        sed -i -e 's/.\[4.m//g' -e 's/.\[3.m//g' -e 's/.\[1K.\[0G/\n/g' "${web_data_dir}/${file}" 2> /dev/null
        sed -i -e 's/.\[1m//g' -e 's/.\[0m//g' "${web_data_dir}/${file}" 2> /dev/null
        sed -i -e '/Last request to/d' "${web_data_dir}/${file}" 2> /dev/null
        #sed -i -e '/^$/d' "${file}" 2> /dev/null
    done 
    echo "Done!"
    unset file
    unset files
       
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Cleaning up gobuster files... "
    for file in $(ls -1 "${web_data_dir}" | grep gobuster); do
        sed -i "s/^..\[2K//" "${web_data_dir}/${file}" 2> /dev/null
    done 
    echo "Done!"
    unset file
    unset files
}

robots_txt(){
    echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for new URLs on robots.txt... "
    #files=($(ls -1 "${web_data_dir}"))
    #for file in "${files[@]}"; do
    for file in $(ls -1 "${web_data_dir}/"); do
        if grep robots.txt "${web_data_dir}/${file}" > /dev/null && [ -s "${file}" ] ; then
            echo "aqui"
            target=$(grep -E "Target:|Url:" "${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//') 
            for url in $(curl -s "${target}"/robots.txt | grep -Ev "User-agent: *" | awk '{print $2}' | sed -e "/^\/$/d"); do
                echo "${target}${url}" >> "${report_dir}/robots_urls.txt"
                sed -i -e 's/\r//g' -e 's/\/$//g' "${report_dir}/robots_urls.txt"
            done
        fi
        unset target
        unset file
    done 
    echo "Done!"
    unset files
}

aquatone_function(){
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Starting aquatone scan... "
    for file in "${report_dir}/domains_web.txt" "${report_dir}/robots_urls.txt"; do
        if [ -s "${file}" ]; then
            aquatone_log=${tmp_dir}/aquatone_$(basename "${file}" | awk -F'.' '{print $1}').log
            aquatone_files_dir=${aquatone_data}/$(basename "${file}" | awk -F'.' '{print $1}')
            if [ ! -d "${aquatone_files_dir}" ]; then
                if mkdir -p "${aquatone_files_dir}" ; then
                    "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
                else
                    echo " "
                    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Something got wrong, wasnt possible create directory ${aquatone_files_dir}."
                    echo -e "\t Please, look what got wrong and run the script again. Stopping the script!"
                    exit 1
                fi
            else
                "${aquatone_bin}" -chrome-path "${chromium_bin}" -out "${aquatone_files_dir}" -threads "${aquatone_threads}" < "${file}" > "${aquatone_log}"
            fi
        else
            echo -e "\t The ${file} does not exist or is empty!"
        fi
    done
    unset aquatone_log
    unset aquatone_files_dir
    unset file
    echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Finish aquatone scan!"
}

rebuild_git(){
    if [ ! -x "${PWD}/git-dumper/git-dumper.py" ]; then
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} The git-dumper.py script does not exist, exiting!"
        exit 1
    else
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Looking for git repository on web_data directory..."
        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} This function has no 100% guaranty to completely recover the .git repository."
        count=1
        proxy_port=8118
        for file in $(ls -1 "${web_data_dir}"); do
            if [[ $(grep -o ".git/config" "${web_data_dir}/${file}") ]]; then
                target_dir="${report_dir}/$(grep -E "Target:|Url:" "${web_data_dir}/${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//' -e 's/http:\/\///' -e 's/https:\/\///')"
                target=$(grep -E "Target:|Url:" "${web_data_dir}/${file}" | sed -e 's/^\[+\] //' | awk '{print $2}' | sed -e 's/\/$//')
                if [ -n "${use_proxy}" ] && [ "${use_proxy}" == "yes" ]; then
                    name_instance="privoxy-${count}"
                    # Running a docker proxy instance to try bypass some protection system like WAF
                    ${docker_bin} run -d --rm --name "${name_instance}" -p "${proxy_port}:8118" privoxy > /dev/null
                    proxy_ip=$(${docker_bin} inspect --format '{{ .NetworkSettings.IPAddress }}' "privoxy-${count}")
                    while [ -z "${Congratulations}" ]; do
                        Congratulations=$(curl -m 2 -s --proxy "${proxy_ip}:8118" 'https://check.torproject.org/' 2> /dev/null | grep -m1 "Congratulations" | awk '{print $1}')
                    done
                    if [[ "200" -eq "$(curl --proxy "${proxy_ip}:8118" -o /dev/null -s -w "%{http_code}\n" "${target}/.git/config")" ]] ; then
                        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Found .git on ${green}${target}${reset}!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Creating the .git directory structure for ${green}${target}${reset}... "
                        echo "Done!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading the static and objects files from repository... "
                        "${PWD}/git-dumper/git-dumper.py" --proxy "${proxy_ip}:8118" "${target}" "${target_dir}" > /dev/null 2>&1
                        echo "Done!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading files from repository... "
                        dir_origem="${PWD}"
                        cd "${target_dir}" || exit
                        for repo_file in $(git ls-files); do
                            repo_file_dir=$(dirname "${repo_file}")
                            if [[ ! -d "${repo_file_dir}" ]] && [[ "${repo_file_dir}" != "." ]]; then
                                mkdir -p "${repo_file_dir}"
                            fi
                            curl -L -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" \
                                --proxy "${proxy_ip}:8118" -f -s -k --max-time 60 "${target}/${repo_file}" -o "${repo_file}" &
                        done    
                        while pgrep -f curl > /dev/null; do
                            sleep 1
                        done
                        echo "Done!"
                        cd "${dir_origem}" || exit
                    fi
                    (( count+=1 ))
                    (( proxy_port+=1 ))
                    ${docker_bin} stop ${name_instance} > /dev/null
                else
                    if [[ "200" -eq "$(curl -o /dev/null -s -w "%{http_code}\n" "${target}/.git/config")" ]] ; then
                        echo -e "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Found .git on ${green}${target}${reset}!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Creating the .git directory structure for ${green}${target}${reset}... "
                        echo "Done!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading the static and objects files from repository... "
                        "${PWD}/git-dumper/git-dumper.py" "${target}" "${target_dir}" > /dev/null 2>&1
                        echo "Done!"
                        echo -ne "${yellow}$(date +%H:%M)${reset} ${red}>>${reset} Downloading files from repository... "
                        dir_origem="${PWD}"
                        cd "${target_dir}" || exit
                        for repo_file in $(git ls-files); do
                            repo_file_dir=$(dirname "${repo_file}")
                            if [[ ! -d "${repo_file_dir}" ]] && [[ "${repo_file_dir}" != "." ]]; then
                                mkdir -p "${repo_file_dir}"
                            fi
                            curl -L -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36" \
                                -f -s -k --max-time 60 "${target}/${repo_file}" -o "${repo_file}" &
                        done
                        while pgrep -f curl > /dev/null; do
                            sleep 1
                        done
                        echo "Done!"
                        cd "${dir_origem}" || exit
                    fi
                fi
            fi
        done
    fi
}

#report(){
#   Falta criar a parte de report, página web, subir o server em python, etc...
#}

# Initiating the recon.sh script
(
# Show the directory structure
echo "./${domain}"
echo -e "  ├── log (${yellow}log dir for recon.sh execution${reset})"
echo -e "  └── $(echo "${recon_dir}" | awk -F "/" '{print $3}')"
echo -e "      ├── aquatone (${yellow}aquatone output files${reset})"
echo -e "      ├── report (${yellow}adjust function output files${reset})"
echo -e "      ├── tmp (${yellow}subdomains recon tmp files${reset})"
echo -e "      ├── wayback-data (${yellow}web data function for waybackurl output${reset})"
echo -e "      └── web-data (${yellow}web data function for gobuster and dirsearch output${reset})"
echo "Directories created."
echo -e "${red}Attention:${reset} The output from all tools used here will be placed in background and treated later."
echo -e "\t   If you need look the output in execution time, you need to \"tail\" the files."

#subdomains_recon
#adjust_files
#diff_domains
nmap_ips
#hosts_alive
#web_data "${report_dir}/domains_web.txt"
#cleanup_web_data_files
#robots_txt
#web_data "${report_dir}/robots_urls.txt"
#cleanup_web_data_files
#aquatone_function
#rebuild_git
#report
message
) 2>&1 | tee -a "${log_dir}/recon_${date_recon}.log"
